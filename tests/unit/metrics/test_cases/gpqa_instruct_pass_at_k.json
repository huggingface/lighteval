{
  "name": "Gpqa Instruct Pass At K Test Suite",
  "description": "Comprehensive test cases for gpqa_instruct_pass_at_k metric covering various scenarios including multiple samples, different k values, and multiple choice letter indices (A, B, C, D, etc.)",
  "test_cases": [
    {
      "name": "Gpqa Instruct Pass At K - Basic Single Sample Correct",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 1,
        "n": 1,
        "strip_strings": true
      },
      "doc": {
        "query": "What is the capital of France?\nA. London\nB. Paris\nC. Berlin\nD. Madrid",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 1,
        "task_name": "geography"
      },
      "model_response": {
        "text": ["B"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=1&n=1&strip_strings=True": 1.0
      },
      "tolerance": 0.01,
      "description": "Basic test case with single correct sample"
    },
    {
      "name": "Gpqa Instruct Pass At K - Multiple Samples All Correct",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 2,
        "n": 3,
        "strip_strings": true
      },
      "doc": {
        "query": "What is the largest planet in our solar system?\nA. Earth\nB. Jupiter\nC. Saturn\nD. Mars",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 1,
        "task_name": "astronomy"
      },
      "model_response": {
        "text": ["B", "B", "B"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=2&n=3&strip_strings=True": 1.0
      },
      "tolerance": 0.01,
      "description": "Test case with multiple samples all correct"
    },
    {
      "name": "Gpqa Instruct Pass At K - Mixed Correct and Incorrect",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 2,
        "n": 4,
        "strip_strings": true
      },
      "doc": {
        "query": "Who wrote Romeo and Juliet?\nA. Charles Dickens\nB. William Shakespeare\nC. Jane Austen\nD. Mark Twain",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 1,
        "task_name": "literature"
      },
      "model_response": {
        "text": ["B", "A", "B", "C"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=2&n=4&strip_strings=True": 0.8333333333333333
      },
      "tolerance": 0.01,
      "description": "Test case with mixed correct and incorrect samples"
    },
    {
      "name": "Gpqa Instruct Pass At K - Case Sensitivity",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 1,
        "n": 2,
        "strip_strings": true
      },
      "doc": {
        "query": "What is the chemical symbol for gold?\nA. Ag\nB. Au\nC. Fe\nD. Cu",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 1,
        "task_name": "chemistry"
      },
      "model_response": {
        "text": ["B", "b"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=1&n=2&strip_strings=True": 0.5
      },
      "tolerance": 0.01,
      "description": "Test case with case sensitivity (strip_strings should handle this)"
    },
    {
      "name": "Gpqa Instruct Pass At K - All Incorrect Samples",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 1,
        "n": 3,
        "strip_strings": true
      },
      "doc": {
        "query": "What year did World War II end?\nA. 1943\nB. 1944\nC. 1945\nD. 1946",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 2,
        "task_name": "history"
      },
      "model_response": {
        "text": ["A", "B", "D"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=1&n=3&strip_strings=True": 0.0
      },
      "tolerance": 0.01,
      "description": "Test case with all incorrect samples"
    },
    {
      "name": "Gpqa Instruct Pass At K - High K Value",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 5,
        "n": 8,
        "strip_strings": true
      },
      "doc": {
        "query": "What is the speed of light in vacuum?\nA. 299,792,458 m/s\nB. 300,000 km/s\nC. 186,282 miles/s\nD. 3x10^8 m/s",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 0,
        "task_name": "physics"
      },
      "model_response": {
        "text": ["A", "B", "A", "C", "A", "D", "A", "B"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=5&n=8&strip_strings=True": 1.0
      },
      "tolerance": 0.01,
      "description": "Test case with high k value and multiple correct samples"
    },
    {
      "name": "Gpqa Instruct Pass At K - Parentheses Format",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 1,
        "n": 2,
        "strip_strings": true
      },
      "doc": {
        "query": "What is the main theme of George Orwell's 1984?\nA. Love and romance\nB. Totalitarianism and surveillance\nC. War and peace\nD. Economic inequality",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 1,
        "task_name": "literature"
      },
      "model_response": {
        "text": ["(B)", "B"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=1&n=2&strip_strings=True": 1.0
      },
      "tolerance": 0.01,
      "description": "Test case with parentheses format"
    },
    {
      "name": "Gpqa Instruct Pass At K - Reasoning with Answer",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 1,
        "n": 2,
        "strip_strings": true
      },
      "doc": {
        "query": "How many sides does a hexagon have?\nA. 4\nB. 5\nC. 6\nD. 7",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 2,
        "task_name": "geometry"
      },
      "model_response": {
        "text": ["A hexagon has 6 sides, so the answer is C", "C"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=1&n=2&strip_strings=True": 1.0
      },
      "tolerance": 0.01,
      "description": "Test case with reasoning and answer extraction"
    },
    {
      "name": "Gpqa Instruct Pass At K - Final Answer Format",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 1,
        "n": 2,
        "strip_strings": true
      },
      "doc": {
        "query": "What is the largest ocean on Earth?\nA. Atlantic Ocean\nB. Indian Ocean\nC. Pacific Ocean\nD. Arctic Ocean",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 2,
        "task_name": "geography"
      },
      "model_response": {
        "text": ["The largest ocean is the Pacific Ocean. Final answer is C", "C"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=1&n=2&strip_strings=True": 1.0
      },
      "tolerance": 0.01,
      "description": "Test case with 'final answer' format"
    },
    {
      "name": "Gpqa Instruct Pass At K - Edge Case Single Choice",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 1,
        "n": 1,
        "strip_strings": true
      },
      "doc": {
        "query": "Is the Earth round?\nA. Yes",
        "choices": ["A"],
        "gold_index": 0,
        "task_name": "science"
      },
      "model_response": {
        "text": ["A"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=1&n=1&strip_strings=True": 1.0
      },
      "tolerance": 0.01,
      "description": "Edge case with single choice"
    },
    {
      "name": "Gpqa Instruct Pass At K - Multiple Correct Answers",
      "metric_class": "gpqa_instruct_pass_at_k",
      "metric_params": {
        "k": 2,
        "n": 4,
        "strip_strings": true
      },
      "doc": {
        "query": "Which of the following are primary colors?\nA. Red\nB. Blue\nC. Green\nD. Yellow",
        "choices": ["A", "B", "C", "D"],
        "gold_index": 0,
        "task_name": "art"
      },
      "model_response": {
        "text": ["A", "B", "A", "C"],
        "logprobs": [],
        "output_tokens": []
      },
      "expected_output": {
        "gpqa_pass@k:k=2&n=4&strip_strings=True": 0.8333333333333333
      },
      "tolerance": 0.01,
      "description": "Test case with multiple correct answers (first correct answer)"
    }
  ]
}
