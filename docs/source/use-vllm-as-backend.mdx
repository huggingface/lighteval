# Use VLLM as backend

Lighteval allows you to use `vllm` as backend allowing great speedups.
To use, simply change the `model_args` to reflect the arguments you want to pass to vllm.

```bash
lighteval vllm \
    "pretrained=HuggingFaceH4/zephyr-7b-beta,dtype=float16" \
    "leaderboard|truthfulqa:mc|0|0"
```

`vllm` is able to distribute the model across multiple GPUs using data
parallelism, pipeline parallelism or tensor parallelism.
You can choose the parallelism method by setting in the the `model_args`.

For example if you have 4 GPUs you can split it across using `tensor_parallelism`:

```bash
export VLLM_WORKER_MULTIPROC_METHOD=spawn && lighteval vllm \
    "pretrained=HuggingFaceH4/zephyr-7b-beta,dtype=float16,tensor_parallel_size=4" \
    "leaderboard|truthfulqa:mc|0|0"
```

Or, if your model fits on a single GPU, you can use `data_parallelism` to speed up the evaluation:

```bash
lighteval vllm \
    "pretrained=HuggingFaceH4/zephyr-7b-beta,dtype=float16,data_parallel_size=4" \
    "leaderboard|truthfulqa:mc|0|0"
```

## Use a config file

For more advanced configurations, you can use a config file for the model.
An example of a config file is shown below and can be found at `examples/model_configs/vllm_model_config.yaml`.

```bash
lighteval vllm \
    "examples/model_configs/vllm_model_config.yaml" \
    "leaderboard|truthfulqa:mc|0|0"
```

```yaml
model: # Model specific parameters
  base_params:
    model_args: "pretrained=HuggingFaceTB/SmolLM-1.7B,revision=main,dtype=bfloat16" # Model args that you would pass in the command line
  generation: # Generation specific parameters
    temperature: 0.3
    repetition_penalty: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
    seed: 42
    top_k: 0
    min_p: 0.0
    top_p: 0.9
```

> [!WARNING]
> In the case of OOM issues, you might need to reduce the context size of the
> model as well as reduce the `gpu_memory_utilization` parameter.


## Dynamically changing the metric configuration

For special kinds of metrics like `Pass@K` or LiveCodeBench's `codegen` metric, you may need to pass specific values like the number of
generations. This can be done in the `yaml` file in the following way:

```yaml
model: # Model specific parameters
  base_params:
    model_args: "pretrained=HuggingFaceTB/SmolLM-1.7B,revision=main,dtype=bfloat16" # Model args that you would pass in the command line
  generation: # Generation specific parameters
    temperature: 0.3
    repetition_penalty: 1.0
    frequency_penalty: 0.0
    presence_penalty: 0.0
    seed: 42
    top_k: 0
    min_p: 0.0
    top_p: 0.9
  metric_options: # Optional metric arguments
    codegen_pass@1:16:
      num_samples: 16
```

An optional key `metric_options` can be passed in the yaml file,
using the name of the metric or metrics, as defined in the `Metric.metric_name`.
In this case, the `codegen_pass@1:16` metric defined in our tasks will have the `num_samples` updated to 16,
independently of  the number defined by default.


## Multi-node vLLM

It is entirely possible to use vLLM in a multi-node setting. For this, we will use Ray.
In these examples, we will assume that we are on a Slurm cluster where nodes do not have internet access.
Those scripts are heavily inspired by [https://github.com/NERSC/slurm-ray-cluster/](https://github.com/NERSC/slurm-ray-cluster/).

First you need to start the Ray cluster. It has one master, and you need to find an available
port on this node

```bash
function find_available_port {
    printf $(python -c 'import socket; s=socket.socket(); s.bind(("", 0)); print(s.getsockname()[1]); s.close()')
}

PORT=$(find_available_port)
NODELIST=($(scontrol show hostnames $SLURM_JOB_NODELIST))
MASTER=${NODELIST[0]}  # Name of master node
MASTER_IP=$(hostname --ip-address)  # IP address of master node

function set_VLLM_HOST_IP {
    export VLLM_HOST_IP=$(hostname --ip-address);
}
export -f set_VLLM_HOST_IP;

# Start the master
srun -N1 -n1 -c $(( SLURM_CPUS_PER_TASK/2 )) -w $MASTER bash -c "set_VLLM_HOST_IP; ray start --head --port=$PORT --block" &
sleep 5

# Start all other nodes :)
if [[ $SLURM_NNODES -gt 1 ]]; then
    srun -N $(( SLURM_NNODES-1 )) --ntasks-per-node=1 -c $(( SLURM_CPUS_PER_TASK/2 )) -x $MASTER bash -c "set_VLLM_HOST_IP; ray start --address=$MASTER_IP:$PORT --block" &
    sleep 5
fi
```

Then, once the Ray cluster is running, you can launch vLLM through lighteval.

```bash
set_VLLM_HOST_IP

MODEL_ARGS="pretrained=$MODEL_DIRECTORY,gpu_memory_utilisation=0.5,trust_remote_code=False,dtype=bfloat16,max_model_length=16384,tensor_parallel_size=<gpus_per_node * nnodes>"
TASK_ARGS="community|gpqa-fr|0|0,community|ifeval-fr|0|0"

srun -N1 -n1 -c $(( SLURM_CPUS_PER_TASK/2 )) --overlap -w $MASTER lighteval vllm "$MODEL_ARGS" "$TASK_ARGS" --custom-tasks $TASK_FILE
```

The full script is available [here](https://github.com/huggingface/lighteval/blob/main/examples/slurm/multi_node_vllm.slurm)

## With vLLM Serve

It is also possible to use the vLLM serve command to achieve a similar result.
It has the following benefits: can be queried by multiple jobs, can be launched only once when needing multiple evaluation,
has lower peak memory on rank 0.

We also need to start the Ray cluster, the exact same way as before. However, now,
before calling lighteval, we need to start our vllm server.

```bash
MODEL_NAME="Llama-3.2-1B-Instruct"
SERVER_PORT=$(find_available_port)
export OPENAI_API_KEY="I-love-vLLM"
export OPENAI_BASE_URL="http://localhost:$SERVER_PORT/v1"

vllm serve $MODEL_DIRECTORY \
    --served-model-name $MODEL_NAME \
    --api-key $OPENAI_API_KEY \
    --enforce-eager \
    --port $SERVER_PORT \
    --tensor-parallel-size <gpus_per_node * nnodes> \
    --dtype bfloat16 \
    --max-model-len 16384 \
    --gpu-memory-utilization 0.8 \
    --disable-custom-all-reduce \
    1>vllm.stdout 2>vllm.stderr &
```

In my case, I want the evaluation to be done within the same job, hence why vllm serve
is put in the background. Therefore, we need to wait until it is up & running before
launching lighteval

```bash
ATTEMPT=0
DELAY=5
MAX_ATTEMPTS=60 # Might need to be increased in case of very large model
until curl -s -o /dev/null -w "%{http_code}" $OPENAI_BASE_URL/models -H "Authorization: Bearer $OPENAI_API_KEY" | grep -E "^2[0-9]{2}$"; do
    ATTEMPT=$((ATTEMPT + 1))
    echo "$ATTEMPT attempts"
    if [ "$ATTEMPT" -ge "$MAX_ATTEMPTS" ]; then
        echo "Failed: the server did not respond any of the $MAX_ATTEMPTS requests."
        exit 1
    fi
    echo "vllm serve is not ready yet"
    sleep $DELAY
done
```

Finally, the above script only finishes when vllm serve is ready, so we can launch
the evaluation.

```bash
export TOKENIZER_PATH="$MODEL_DIRECTORY"

TASK_ARGS="community|gpqa-fr|0|0,community|ifeval-fr|0|0"

lighteval endpoint openai "$MODEL_NAME" "$TASK_ARGS" --custom-tasks $TASK_FILE
```

The full script is available [here](https://github.com/huggingface/lighteval/blob/main/examples/slurm/multi_node_vllm_serve.slurm).

