# üå§Ô∏è Lighteval

**Lighteval** is your all-in-one toolkit for evaluating LLMs across multiple
backends‚Äîwhether it's
[transformers](https://github.com/huggingface/transformers),
[tgi](https://github.com/huggingface/text-generation-inference),
[vllm](https://github.com/vllm-project/vllm), or
[nanotron](https://github.com/huggingface/nanotron)‚Äîwith
ease. Dive deep into your model‚Äôs performance by saving and exploring detailed,
sample-by-sample results to debug and see how your models stack-up.

Customization at your fingertips: letting you effortlessly create [new
tasks](https://github.com/huggingface/lighteval/wiki/Adding-a-Custom-Task) and
[metrics](https://github.com/huggingface/lighteval/wiki/Adding-a-New-Metric)
tailored to your needs, or browsing all our existing tasks and metrics.

Seamlessly experiment, benchmark, and store your results on the Hugging Face
Hub, S3, or locally.
