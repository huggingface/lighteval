lighteval|arc:challenge|25
lighteval|truthfulqa:mc|0
lighteval|hellaswag|10
lighteval|mmlu:college_chemistry|5
lighteval|mmlu:us_foreign_policy|5
lighteval|agieval:aqua-rat|0
lighteval|agieval:logiqa-en|0
lighteval|agieval:lsat-ar|0
lighteval|agieval:lsat-lr|0
lighteval|agieval:lsat-rc|0
lighteval|agieval:sat-en-without-passage|0
lighteval|agieval:sat-en|0
lighteval|bigbench_hard:causal_judgment|3
lighteval|bigbench_hard:date_understanding|3
lighteval|bigbench_hard:disambiguation_qa|3
lighteval|bigbench_hard:geometric_shapes|3
lighteval|bigbench_hard:logical_deduction_five_objects|3
lighteval|bigbench_hard:logical_deduction_seven_objects|3
lighteval|bigbench_hard:movie_recommendation|3
lighteval|bigbench_hard:navigate|3
lighteval|bigbench_hard:ruin_names|3
lighteval|bigbench_hard:salient_translation_error_detection|3
lighteval|bigbench_hard:snarks|3
lighteval|bigbench_hard:temporal_sequences|3
lighteval|bigbench_hard:tracking_shuffled_objects_five_objects|3
lighteval|bigbench_hard:tracking_shuffled_objects_seven_objects|3
lighteval|gsm8k_test|0
