arc:challenge|25
truthfulqa:mc|0
hellaswag|10
mmlu:college_chemistry|5
mmlu:us_foreign_policy|5
agieval:aqua-rat|0
agieval:logiqa-en|0
agieval:lsat-ar|0
agieval:lsat-lr|0
agieval:lsat-rc|0
agieval:sat-en-without-passage|0
agieval:sat-en|0
bigbench_hard:causal_judgment|3
bigbench_hard:date_understanding|3
bigbench_hard:disambiguation_qa|3
bigbench_hard:geometric_shapes|3
bigbench_hard:logical_deduction_five_objects|3
bigbench_hard:logical_deduction_seven_objects|3
bigbench_hard:movie_recommendation|3
bigbench_hard:navigate|3
bigbench_hard:ruin_names|3
bigbench_hard:salient_translation_error_detection|3
bigbench_hard:snarks|3
bigbench_hard:temporal_sequences|3
bigbench_hard:tracking_shuffled_objects_five_objects|3
bigbench_hard:tracking_shuffled_objects_seven_objects|3
gsm8k_test|0
