model:
  type: "endpoint" # can be base, tgi, or endpoint
  base_params:
    model_name: "meta-llama/Llama-2-7b-hf" # the model name or the endpoint name if reuse_existing is true
    revision: "main"
    dtype: "float16" # can be any of "awq", "eetq", "gptq", "4bit' or "8bit" (will use bitsandbytes), "bfloat16" or "float16"
    reuse_existing: false # if true, ignore all params in instance, and don't delete the endpoint after evaluation
  instance:
    accelerator: "gpu"
    region: "eu-west-1"
    vendor: "aws"
    instance_size: "x1"
    instance_type: "nvidia-a10g"
    framework: "pytorch"
    endpoint_type: "protected"
    namespace: null # The namespace under which to launch the endopint. Defaults to the current user's namespace
    image_url: null # Optionally specify the docker image to use when launching the endpoint model. E.g., launching models with later releases of the TGI container with support for newer models.
    env_vars:
      null # Optional environment variables to include when launching the endpoint. e.g., `MAX_INPUT_LENGTH: 2048`
  generation:
    add_special_tokens: true
