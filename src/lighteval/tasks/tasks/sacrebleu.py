# MIT License

# Copyright (c) 2024 The HuggingFace Team

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import lighteval.tasks.default_prompts as prompt
from lighteval.metrics.metrics import Metrics
from lighteval.tasks.lighteval_task import LightevalTaskConfig


iwslt17_ar_en_lighteval = LightevalTaskConfig(
    name="iwslt17:ar-en",
    suite=["lighteval", "harness_selection"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_ar-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_de_en_lighteval = LightevalTaskConfig(
    name="iwslt17:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_en_ar_lighteval = LightevalTaskConfig(
    name="iwslt17:en-ar",
    suite=["lighteval", "harness_selection"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_ar-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_en_de_lighteval = LightevalTaskConfig(
    name="iwslt17:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_en_fr_lighteval = LightevalTaskConfig(
    name="iwslt17:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_en_ja_lighteval = LightevalTaskConfig(
    name="iwslt17:en-ja",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_en-ja",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_en_ko_lighteval = LightevalTaskConfig(
    name="iwslt17:en-ko",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_en-ko",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_en_zh_lighteval = LightevalTaskConfig(
    name="iwslt17:en-zh",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_en-zh",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_fr_en_lighteval = LightevalTaskConfig(
    name="iwslt17:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_ja_en_lighteval = LightevalTaskConfig(
    name="iwslt17:ja-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_ja-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_ko_en_lighteval = LightevalTaskConfig(
    name="iwslt17:ko-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_ko-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

iwslt17_zh_en_lighteval = LightevalTaskConfig(
    name="iwslt17:zh-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="iwslt17_zh-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

mtnt2019_en_fr_lighteval = LightevalTaskConfig(
    name="mtnt2019:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="mtnt2019_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=200,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

mtnt2019_en_ja_lighteval = LightevalTaskConfig(
    name="mtnt2019:en-ja",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="mtnt2019_en-ja",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=200,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

mtnt2019_fr_en_lighteval = LightevalTaskConfig(
    name="mtnt2019:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="mtnt2019_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=200,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

mtnt2019_ja_en_lighteval = LightevalTaskConfig(
    name="mtnt2019:ja-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="mtnt2019_ja-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=200,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_cs_en_lighteval = LightevalTaskConfig(
    name="wmt08:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_de_en_lighteval = LightevalTaskConfig(
    name="wmt08:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_en_cs_lighteval = LightevalTaskConfig(
    name="wmt08:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_en_de_lighteval = LightevalTaskConfig(
    name="wmt08:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_en_es_lighteval = LightevalTaskConfig(
    name="wmt08:en-es",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_en-es",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_en_fr_lighteval = LightevalTaskConfig(
    name="wmt08:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_en_hu_lighteval = LightevalTaskConfig(
    name="wmt08:en-hu",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_en-hu",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_es_en_lighteval = LightevalTaskConfig(
    name="wmt08:es-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_es-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_fr_en_lighteval = LightevalTaskConfig(
    name="wmt08:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt08_hu_en_lighteval = LightevalTaskConfig(
    name="wmt08:hu-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt08_hu-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_cs_en_lighteval = LightevalTaskConfig(
    name="wmt09:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_de_en_lighteval = LightevalTaskConfig(
    name="wmt09:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_en_cs_lighteval = LightevalTaskConfig(
    name="wmt09:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_en_de_lighteval = LightevalTaskConfig(
    name="wmt09:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_en_es_lighteval = LightevalTaskConfig(
    name="wmt09:en-es",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_en-es",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_en_fr_lighteval = LightevalTaskConfig(
    name="wmt09:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_en_hu_lighteval = LightevalTaskConfig(
    name="wmt09:en-hu",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_en-hu",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_en_it_lighteval = LightevalTaskConfig(
    name="wmt09:en-it",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_en-it",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_es_en_lighteval = LightevalTaskConfig(
    name="wmt09:es-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_es-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_fr_en_lighteval = LightevalTaskConfig(
    name="wmt09:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_hu_en_lighteval = LightevalTaskConfig(
    name="wmt09:hu-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_hu-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt09_it_en_lighteval = LightevalTaskConfig(
    name="wmt09:it-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt09_it-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt10_cs_en_lighteval = LightevalTaskConfig(
    name="wmt10:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt10_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt10_de_en_lighteval = LightevalTaskConfig(
    name="wmt10:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt10_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt10_en_cs_lighteval = LightevalTaskConfig(
    name="wmt10:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt10_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt10_en_de_lighteval = LightevalTaskConfig(
    name="wmt10:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt10_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt10_en_es_lighteval = LightevalTaskConfig(
    name="wmt10:en-es",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt10_en-es",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt10_en_fr_lighteval = LightevalTaskConfig(
    name="wmt10:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt10_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt10_es_en_lighteval = LightevalTaskConfig(
    name="wmt10:es-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt10_es-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt10_fr_en_lighteval = LightevalTaskConfig(
    name="wmt10:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt10_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt11_cs_en_lighteval = LightevalTaskConfig(
    name="wmt11:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt11_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt11_de_en_lighteval = LightevalTaskConfig(
    name="wmt11:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt11_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt11_en_cs_lighteval = LightevalTaskConfig(
    name="wmt11:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt11_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt11_en_de_lighteval = LightevalTaskConfig(
    name="wmt11:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt11_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt11_en_es_lighteval = LightevalTaskConfig(
    name="wmt11:en-es",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt11_en-es",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt11_en_fr_lighteval = LightevalTaskConfig(
    name="wmt11:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt11_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt11_es_en_lighteval = LightevalTaskConfig(
    name="wmt11:es-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt11_es-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt11_fr_en_lighteval = LightevalTaskConfig(
    name="wmt11:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt11_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt12_cs_en_lighteval = LightevalTaskConfig(
    name="wmt12:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt12_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt12_de_en_lighteval = LightevalTaskConfig(
    name="wmt12:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt12_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt12_en_cs_lighteval = LightevalTaskConfig(
    name="wmt12:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt12_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt12_en_de_lighteval = LightevalTaskConfig(
    name="wmt12:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt12_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt12_en_es_lighteval = LightevalTaskConfig(
    name="wmt12:en-es",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt12_en-es",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt12_en_fr_lighteval = LightevalTaskConfig(
    name="wmt12:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt12_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt12_es_en_lighteval = LightevalTaskConfig(
    name="wmt12:es-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt12_es-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt12_fr_en_lighteval = LightevalTaskConfig(
    name="wmt12:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt12_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_cs_en_lighteval = LightevalTaskConfig(
    name="wmt13:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_de_en_lighteval = LightevalTaskConfig(
    name="wmt13:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_en_cs_lighteval = LightevalTaskConfig(
    name="wmt13:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_en_de_lighteval = LightevalTaskConfig(
    name="wmt13:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_en_es_lighteval = LightevalTaskConfig(
    name="wmt13:en-es",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_en-es",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_en_fr_lighteval = LightevalTaskConfig(
    name="wmt13:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_en_ru_lighteval = LightevalTaskConfig(
    name="wmt13:en-ru",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_en-ru",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_es_en_lighteval = LightevalTaskConfig(
    name="wmt13:es-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_es-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_fr_en_lighteval = LightevalTaskConfig(
    name="wmt13:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt13_ru_en_lighteval = LightevalTaskConfig(
    name="wmt13:ru-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt13_ru-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_cs_en_lighteval = LightevalTaskConfig(
    name="wmt14:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_de_en_lighteval = LightevalTaskConfig(
    name="wmt14:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_en_cs_lighteval = LightevalTaskConfig(
    name="wmt14:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_en_de_lighteval = LightevalTaskConfig(
    name="wmt14:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_en_fr_lighteval = LightevalTaskConfig(
    name="wmt14:en-fr",
    suite=["lighteval", "gpt3_benchmarks"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="wmt14",
    hf_subset="fr-en",
    hf_avail_splits=["train", "validation", "test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_en_fr_lighteval = LightevalTaskConfig(
    name="wmt14:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_en_hi_lighteval = LightevalTaskConfig(
    name="wmt14:en-hi",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_en-hi",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_en_ru_lighteval = LightevalTaskConfig(
    name="wmt14:en-ru",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_en-ru",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_fr_en_lighteval = LightevalTaskConfig(
    name="wmt14:fr-en",
    suite=["lighteval", "gpt3_benchmarks"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="wmt14",
    hf_subset="fr-en",
    hf_avail_splits=["train", "validation", "test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_fr_en_lighteval = LightevalTaskConfig(
    name="wmt14:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_hi_en_lighteval = LightevalTaskConfig(
    name="wmt14:hi-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_hi-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt14_ru_en_lighteval = LightevalTaskConfig(
    name="wmt14:ru-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt14_ru-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_cs_en_lighteval = LightevalTaskConfig(
    name="wmt15:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_de_en_lighteval = LightevalTaskConfig(
    name="wmt15:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_en_cs_lighteval = LightevalTaskConfig(
    name="wmt15:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_en_de_lighteval = LightevalTaskConfig(
    name="wmt15:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_en_fi_lighteval = LightevalTaskConfig(
    name="wmt15:en-fi",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_en-fi",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_en_fr_lighteval = LightevalTaskConfig(
    name="wmt15:en-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_en-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_en_ru_lighteval = LightevalTaskConfig(
    name="wmt15:en-ru",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_en-ru",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_fi_en_lighteval = LightevalTaskConfig(
    name="wmt15:fi-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_fi-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_fr_en_lighteval = LightevalTaskConfig(
    name="wmt15:fr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_fr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt15_ru_en_lighteval = LightevalTaskConfig(
    name="wmt15:ru-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt15_ru-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_cs_en_lighteval = LightevalTaskConfig(
    name="wmt16:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_de_en_lighteval = LightevalTaskConfig(
    name="wmt16:de-en",
    suite=["lighteval", "gpt3_benchmarks"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="wmt16",
    hf_subset="de-en",
    hf_avail_splits=["train", "validation", "test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_de_en_lighteval = LightevalTaskConfig(
    name="wmt16:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_en_cs_lighteval = LightevalTaskConfig(
    name="wmt16:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_en_de_lighteval = LightevalTaskConfig(
    name="wmt16:en-de",
    suite=["lighteval", "gpt3_benchmarks"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="wmt16",
    hf_subset="de-en",
    hf_avail_splits=["train", "validation", "test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_en_de_lighteval = LightevalTaskConfig(
    name="wmt16:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_en_fi_lighteval = LightevalTaskConfig(
    name="wmt16:en-fi",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_en-fi",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_en_ro_lighteval = LightevalTaskConfig(
    name="wmt16:en-ro",
    suite=["lighteval", "gpt3_benchmarks"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="wmt16",
    hf_subset="ro-en",
    hf_avail_splits=["train", "validation", "test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_en_ro_lighteval = LightevalTaskConfig(
    name="wmt16:en-ro",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_en-ro",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_en_ru_lighteval = LightevalTaskConfig(
    name="wmt16:en-ru",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_en-ru",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_en_tr_lighteval = LightevalTaskConfig(
    name="wmt16:en-tr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_en-tr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_fi_en_lighteval = LightevalTaskConfig(
    name="wmt16:fi-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_fi-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_ro_en_lighteval = LightevalTaskConfig(
    name="wmt16:ro-en",
    suite=["lighteval", "gpt3_benchmarks"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="wmt16",
    hf_subset="ro-en",
    hf_avail_splits=["train", "validation", "test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_ro_en_lighteval = LightevalTaskConfig(
    name="wmt16:ro-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_ro-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_ru_en_lighteval = LightevalTaskConfig(
    name="wmt16:ru-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_ru-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt16_tr_en_lighteval = LightevalTaskConfig(
    name="wmt16:tr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt16_tr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_cs_en_lighteval = LightevalTaskConfig(
    name="wmt17:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_de_en_lighteval = LightevalTaskConfig(
    name="wmt17:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_en_cs_lighteval = LightevalTaskConfig(
    name="wmt17:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_en_de_lighteval = LightevalTaskConfig(
    name="wmt17:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_en_fi_lighteval = LightevalTaskConfig(
    name="wmt17:en-fi",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_en-fi",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_en_lv_lighteval = LightevalTaskConfig(
    name="wmt17:en-lv",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_en-lv",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_en_ru_lighteval = LightevalTaskConfig(
    name="wmt17:en-ru",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_en-ru",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_en_tr_lighteval = LightevalTaskConfig(
    name="wmt17:en-tr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_en-tr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_en_zh_lighteval = LightevalTaskConfig(
    name="wmt17:en-zh",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_en-zh",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_fi_en_lighteval = LightevalTaskConfig(
    name="wmt17:fi-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_fi-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_lv_en_lighteval = LightevalTaskConfig(
    name="wmt17:lv-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_lv-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_ru_en_lighteval = LightevalTaskConfig(
    name="wmt17:ru-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_ru-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_tr_en_lighteval = LightevalTaskConfig(
    name="wmt17:tr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_tr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt17_zh_en_lighteval = LightevalTaskConfig(
    name="wmt17:zh-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt17_zh-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_cs_en_lighteval = LightevalTaskConfig(
    name="wmt18:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_de_en_lighteval = LightevalTaskConfig(
    name="wmt18:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_en_cs_lighteval = LightevalTaskConfig(
    name="wmt18:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_en_de_lighteval = LightevalTaskConfig(
    name="wmt18:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_en_et_lighteval = LightevalTaskConfig(
    name="wmt18:en-et",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_en-et",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_en_fi_lighteval = LightevalTaskConfig(
    name="wmt18:en-fi",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_en-fi",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_en_ru_lighteval = LightevalTaskConfig(
    name="wmt18:en-ru",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_en-ru",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_en_tr_lighteval = LightevalTaskConfig(
    name="wmt18:en-tr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_en-tr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_en_zh_lighteval = LightevalTaskConfig(
    name="wmt18:en-zh",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_en-zh",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_et_en_lighteval = LightevalTaskConfig(
    name="wmt18:et-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_et-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_fi_en_lighteval = LightevalTaskConfig(
    name="wmt18:fi-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_fi-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_ru_en_lighteval = LightevalTaskConfig(
    name="wmt18:ru-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_ru-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_tr_en_lighteval = LightevalTaskConfig(
    name="wmt18:tr-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_tr-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt18_zh_en_lighteval = LightevalTaskConfig(
    name="wmt18:zh-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt18_zh-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_cs_de_lighteval = LightevalTaskConfig(
    name="wmt19:cs-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_cs-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_de_cs_lighteval = LightevalTaskConfig(
    name="wmt19:de-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_de-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_de_en_lighteval = LightevalTaskConfig(
    name="wmt19:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_de_fr_lighteval = LightevalTaskConfig(
    name="wmt19:de-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_de-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_en_cs_lighteval = LightevalTaskConfig(
    name="wmt19:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_en_de_lighteval = LightevalTaskConfig(
    name="wmt19:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_en_fi_lighteval = LightevalTaskConfig(
    name="wmt19:en-fi",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_en-fi",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_en_gu_lighteval = LightevalTaskConfig(
    name="wmt19:en-gu",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_en-gu",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_en_kk_lighteval = LightevalTaskConfig(
    name="wmt19:en-kk",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_en-kk",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_en_lt_lighteval = LightevalTaskConfig(
    name="wmt19:en-lt",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_en-lt",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_en_ru_lighteval = LightevalTaskConfig(
    name="wmt19:en-ru",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_en-ru",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_en_zh_lighteval = LightevalTaskConfig(
    name="wmt19:en-zh",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_en-zh",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_fi_en_lighteval = LightevalTaskConfig(
    name="wmt19:fi-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_fi-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_fr_de_lighteval = LightevalTaskConfig(
    name="wmt19:fr-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_fr-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_gu_en_lighteval = LightevalTaskConfig(
    name="wmt19:gu-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_gu-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_kk_en_lighteval = LightevalTaskConfig(
    name="wmt19:kk-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_kk-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_lt_en_lighteval = LightevalTaskConfig(
    name="wmt19:lt-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_lt-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_ru_en_lighteval = LightevalTaskConfig(
    name="wmt19:ru-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_ru-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt19_zh_en_lighteval = LightevalTaskConfig(
    name="wmt19:zh-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt19_zh-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_cs_en_lighteval = LightevalTaskConfig(
    name="wmt20:cs-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_cs-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_de_en_lighteval = LightevalTaskConfig(
    name="wmt20:de-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_de-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_de_fr_lighteval = LightevalTaskConfig(
    name="wmt20:de-fr",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_de-fr",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_cs_lighteval = LightevalTaskConfig(
    name="wmt20:en-cs",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-cs",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_de_lighteval = LightevalTaskConfig(
    name="wmt20:en-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_iu_lighteval = LightevalTaskConfig(
    name="wmt20:en-iu",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-iu",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_ja_lighteval = LightevalTaskConfig(
    name="wmt20:en-ja",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-ja",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_km_lighteval = LightevalTaskConfig(
    name="wmt20:en-km",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-km",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_pl_lighteval = LightevalTaskConfig(
    name="wmt20:en-pl",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-pl",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_ps_lighteval = LightevalTaskConfig(
    name="wmt20:en-ps",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-ps",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_ru_lighteval = LightevalTaskConfig(
    name="wmt20:en-ru",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-ru",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_ta_lighteval = LightevalTaskConfig(
    name="wmt20:en-ta",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-ta",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_en_zh_lighteval = LightevalTaskConfig(
    name="wmt20:en-zh",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_en-zh",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_fr_de_lighteval = LightevalTaskConfig(
    name="wmt20:fr-de",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_fr-de",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_iu_en_lighteval = LightevalTaskConfig(
    name="wmt20:iu-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_iu-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_ja_en_lighteval = LightevalTaskConfig(
    name="wmt20:ja-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_ja-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_km_en_lighteval = LightevalTaskConfig(
    name="wmt20:km-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_km-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_pl_en_lighteval = LightevalTaskConfig(
    name="wmt20:pl-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_pl-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_ps_en_lighteval = LightevalTaskConfig(
    name="wmt20:ps-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_ps-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_ru_en_lighteval = LightevalTaskConfig(
    name="wmt20:ru-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_ru-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_ta_en_lighteval = LightevalTaskConfig(
    name="wmt20:ta-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_ta-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)

wmt20_zh_en_lighteval = LightevalTaskConfig(
    name="wmt20:zh-en",
    suite=["lighteval", "sacrebleu"],
    prompt_function=prompt.wmt_reverse_alphabetical,
    hf_repo="lighteval/sacrebleu_manual",
    hf_subset="wmt20_zh-en",
    hf_avail_splits=["test"],
    evaluation_splits=["test"],
    few_shots_split=None,
    few_shots_select=None,
    generation_size=None,
    metrics=[Metrics.bleu, Metrics.chrf, Metrics.ter],
    stop_sequence=["\n"],
    version=0,
)
